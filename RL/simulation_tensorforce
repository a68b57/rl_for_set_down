import logging

from tensorforce.contrib.openai_gym import OpenAIGym
from tensorforce.agents import DQNAgent, VPGAgent
from tensorforce.execution import Runner

import gym
import RL

import numpy as np

def main():
	# gym_id = 'CartPole-v1'
	gym_id = 'SetDown-v2'

	max_episodes = 30000
	max_timesteps = 200

	summary_dir = './tensorforce_output/log/'
	model_dir = './tensorforce_output/model/'

	env = OpenAIGym(gym_id)

	network_spec = [
		dict(type='dense', size=200, activation='sigmoid'),
	]

	update_mode = dict(
		unit="timesteps",
		batch_size=32,
		frequency=1,
	)

	actions_exploration = dict(
		type='epsilon_anneal',
		initial_epsilon=0.5,
		final_epsilon=0.1,
		timesteps=50000,
	)

	summarizer = dict(
		directory = summary_dir,
		steps=1000,
		labels=['loss', 'configuration']

	)

	saver = dict(
		directory=model_dir,
		steps=10000
	)


	memory = dict(
		type="prioritized_replay",
		capacity=1000,
		include_next_states=True,
		# prioritization_weight=
	)



	optimizer = dict(
		type='adam',
		learning_rate=1e-3
	)

	agent = DQNAgent(
		states=env.states,
		actions=env.actions,
		network=network_spec,
		memory=memory,
		saver=saver,
		summarizer=summarizer,
		actions_exploration=actions_exploration,
		double_q_model=True,
		huber_loss=np.inf,
		target_sync_frequency=10000,
		update_mode=update_mode,
		optimizer=optimizer

	)

	runner = Runner(agent, env)
	report_episodes = 1

	def episode_finished(r):
		if r.episode % report_episodes == 0:
			# logging.debug("Finished episode {ep} after {ts} timesteps".format(ep=r.episode, ts=r.timestep))
			# logging.debug("Episode reward: {}".format(r.episode_rewards[-1]))
			# logging.debug("Average of last 100 rewards: {}".format(sum(r.episode_rewards[-100:]) / 100))
			print("Finished episode {ep} after {ts} timesteps".format(ep=r.episode, ts=r.timestep))
			print("Episode reward: {}".format(r.episode_rewards[-1]))
			print("Average of last 100 rewards: {}".format(sum(r.episode_rewards[-100:]) / 100))

		return True

	print("Starting {agent} for Environment '{env}'".format(agent=agent, env=env))

	runner.run(max_episode_timesteps=max_timesteps, episodes=max_episodes, episode_finished=episode_finished)
	runner.close()

	print("Learning finished. Total episodes: {ep}".format(ep=runner.episode))


if __name__ == '__main__':
	main()